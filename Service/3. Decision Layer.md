# 📡 Module 03: Autonomous AI Agent & Implementation Spec
> **Project:** 6G-Guardian (ISAC-based Contactless Athlete Protection System)  
> **Module:** AI Implementation, Open-Source Stack, RL Control  
> **Version:** 3.0.0 (Implementation-Ready)  
> **Author:** Team BAESTRO, Mobile Engineering Dept. KNU

---

## 1. 서론 (Introduction)

### 1.1. 개요 (Overview)
본 문서는 6G-Guardian 시스템의 두뇌인 **Autonomous AI Agent**의 구체적인 구현 방안을 기술한다.
본 에이전트는 단순한 규칙 기반 시스템이 아닌, **심층 강화학습(Deep RL)**을 통해 인프라를 능동 제어(Active Control)하는 자율 시스템이다. 이를 구현하기 위해 검증된 SOTA(State-of-the-Art) 오픈소스 모델을 기반으로, 6G RF 데이터 특성에 맞춘 **Knowledge Distillation(지식 증류)** 및 **Physics-Informed(물리 정보)** 학습 기법을 적용한다.

---

## 2. AI 모델 구현 상세 (Implementation Specification)

4개의 핵심 모듈에 대한 사용 모델, 라이브러리, 튜닝 전략 명세서이다.

### 2.1. Skeletal Kinematics Module (골격 복원)
불완전하고 희소한(Sparse) 6G 레이더 포인트 클라우드를 3D 골격으로 변환한다.

* **Backbone Model:** **PointNet++** (Feature Extractor) + **DGCNN** (Dynamic Graph CNN).
* **Library:** `PyTorch Geometric (PyG)` / `OpenMMLab MMPose`.
* **Engineering Strategy (Knowledge Distillation):**
    * **Problem:** RF 신호는 광학 카메라보다 해상도가 낮아 골격 추출이 어려움.
    * **Solution:** **'Cross-Modal Distillation'** 기법 적용.
        1.  **Teacher (Optical):** `NTU RGB+D 120` 데이터셋으로 학습된 고정밀 카메라 모델.
        2.  **Student (RF):** 6G 레이더 모델.
        3.  **Process:** Teacher가 추출한 Feature Map을 Student가 모방하도록 Loss Function을 설계하여 학습 효율 극대화.

### 2.2. Micro-Tremor Detection Module (떨림 감지)
마이크로 도플러 스펙트로그램 이미지에서 비정상 주파수 패턴을 분류한다.

* **Backbone Model:** **EfficientNet-V2 (Small)**.
    * *Why?* ResNet 대비 파라미터 수가 적어 0.1ms 초저지연 추론(Edge Inference)에 최적화됨.
* **Library:** `Timm (PyTorch Image Models)`.
* **Engineering Strategy (Data Augmentation):**
    * **Problem:** 실제 근육 경련(Cramp) 데이터 확보의 어려움.
    * **Solution:** **'SpecAugment'** 및 **'Contrastive Learning'** 적용.
        1.  **SpecAugment:** 스펙트로그램의 시간/주파수 축을 마스킹(Masking)하여 데이터 증강.
        2.  **SimCLR:** 라벨이 없는 상태에서 '정상 근육 수축'과 '비정상 떨림'의 특징 차이를 스스로 학습(Self-Supervised)하게 한 후 Fine-tuning.

### 2.3. Physiological Inference Module (심부 온도/수분)
데이터가 없는 영역에서도 물리 법칙을 위배하지 않도록 강제한다.

* **Model Architecture:** **PINN (Physics-Informed Neural Network)**.
* **Library:** `DeepXDE` (Scientific ML Library).
* **Engineering Strategy (Physics Regularization):**
    * **Loss Function Design:** 데이터 오차($MSE_{data}$)에 물리 방정식 잔차($MSE_{physics}$)를 추가.
    $$Loss = MSE_{data} + \lambda \cdot || \mathcal{N}_{Pennes}(T) ||^2$$
    * $\mathcal{N}_{Pennes}$: 펜스 생체 열 방정식(Pennes' Bio-heat Eq) 연산자.
    * 이를 통해 피부 온도($T_s$) 데이터만으로 심부 온도($T_b$)를 정확히 역산출.

### 2.4. Autonomous Control Agent (강화학습 제어)
상황에 따라 6G 빔포밍을 제어하고 경기를 중단시키는 의사결정체.

* **Algorithm:** **PPO (Proximal Policy Optimization)**.
    * *Why?* DQN 대비 학습 안정성이 높고, 연속적인 제어(Continuous Action Space)에 유리함.
* **Library:** `Stable Baselines3` (RL Algorithm) / `Ray RLLib` (Distributed Training).
* **Environment:** `Unity ML-Agents` (Sim-to-Real).
* **Reward System:**
    * **(+) Reward:** 부상 징후 조기 포착 (+10), 정확한 위험도 예측 (+5).
    * **(-) Penalty:** 오작동/False Alarm (-5), 부상 놓침 (-20), 컴퓨팅 자원 과다 사용 (-1).

---

## 3. 에이전트 루프 및 의사결정 (Agent Loop & MDP)

에이전트는 **MDP (Markov Decision Process)** 기반의 폐루프(Closed-loop) 제어를 수행한다.

### Step 1: Observation ($S_t$)
* 4개 모듈(골격, 떨림, 체온, 수분)의 출력 벡터 및 신뢰도(Confidence Score) 수집.
* 상태 벡터 $S_t = [v_{skel}, v_{tremor}, v_{temp}, v_{hyd}, c_{conf}]$.

### Step 2: Policy Reasoning ($A_t \sim \pi(S_t)$)
* PPO 정책 신경망이 현재 상태에서 최적의 행동을 확률적으로 선택.
* **Case A (불확실):** 신뢰도 $c_{conf} < 0.6$ $\rightarrow$ **Action: Active Sensing** (빔포밍 집중, RIS 각도 변경).
* **Case B (위험 감지):** 위험도 $R_{total} > 0.9$ $\rightarrow$ **Action: Game Stop** (심판 알림 전송).

### Step 3: Execution & Feedback
* 선택된 행동을 6G 인프라(RU/MEC)에 명령하고, 변화된 환경($S_{t+1}$)을 다시 관측.

---

## 4. 위험도 산출 로직 (Total Risk Scoring)

최종 판단을 위한 정량적 지표 계산식이다.

$$R_{total}(t) = w_1 \cdot R_{kin}(t) + w_2 \cdot R_{trem}(t) + w_3 \cdot R_{vit}(t)$$

* **$R_{kin}$ (ST-GCN Output):** 관절 과신전 및 비정상 충격량 (0.0~1.0).
* **$R_{trem}$ (EfficientNet Output):** 병리학적 떨림 확률 (Softmax Score).
* **$R_{vit}$ (PINN Output):** 열사병($T_b > 39.5^\circ C$) 및 탈수($\text{Loss} > 2\%$) 위험도.
* **Adaptive Weights ($w_i$):** 강화학습 에이전트가 상황(Context)에 따라 가중치를 동적으로 조절. (예: 충돌 직후엔 $w_1$ 증가, 폭염 시엔 $w_3$ 증가).

---

## 5. 시스템 요구사항 및 기술 스택 (Tech Stack)

실제 구현을 위한 하드웨어 및 소프트웨어 요구사항이다.

### 5.1. Hardware Spec
* **Edge Device (Inference):** NVIDIA Jetson Orin AGX (32GB) or A100 Tensor Core GPU.
* **Latency Target:** End-to-End Processing < **10ms**.

### 5.2. Software Stack (`requirements.txt`)
```text
# Core Framework
torch==2.1.0+cu118
numpy==1.26.0

# Model Implementation
torch-geometric==2.4.0   # for ST-GCN, PointNet++
timm==0.9.12             # for EfficientNet-V2
deepxde==1.10.0          # for PINN (Bio-heat)

# Reinforcement Learning
stable-baselines3==2.2.1 # for PPO Agent
gymnasium==0.29.1        # for RL Environment

# Inference Optimization
tensorrt==8.6.1          # for FP16/INT8 Quantization
