# 📡 Module 03: Autonomous AI Agent & Active Control
> **Project:** 6G-Guardian (ISAC-based Contactless Athlete Protection System)  
> **Module:** Autonomous Agent, Reinforcement Learning, Active Sensing Control  
> **Version:** 2.0.0 (Agentic Architecture Transformation)  
> **Author:** Mobile Engineering Dept. KNU

---

## 1. 서론 (Introduction)

### 1.1. 개요 (Overview)
본 모듈은 단순한 이상 징후 탐지 알고리즘이 아닌, 6G 네트워크 인프라를 능동적으로 제어하고 상황을 통제하는 **자율형 AI 에이전트(Autonomous AI Agent)**에 대해 정의한다.
이 에이전트는 **심층 강화학습(Deep Reinforcement Learning)**을 기반으로 작동하며, 데이터 수신(Perception) $\rightarrow$ 판단(Reasoning) $\rightarrow$ 행동(Action)의 루프를 실시간으로 수행한다. 특히 정보가 불충분할 경우 스스로 **Beamforming을 제어(Active Sensing)**하여 정밀 데이터를 확보하는 주체적인 의사결정권자이다.

---

## 2. 에이전트 아키텍처 (Agent Architecture)

에이전트는 **Perception Engine(감각)**, **Digital Twin(기억)**, **Policy Network(두뇌/행동)**의 3단계 구조로 구성된다.

### 2.1. Perception Engine (감각 기관)
Sensing Layer의 Raw Data를 처리하여 상태(State)를 인지하는 하위 모듈들의 집합이다. (기존 모델 활용)
* **Module A (Skeleton):** ST-GCN (관절 구조 분석).
* **Module B (Tremor):** Transformer (미세 떨림 감지).
* **Module C (Physiology):** PINN (심부 온도 및 수분 역산).
* **Output:** 현재 선수의 상태 벡터 $S_t$ (State Vector).

### 2.2. Policy Network (두뇌: Reinforcement Learning)
현재 상태 $S_t$를 바탕으로 최적의 행동 $A_t$를 결정하는 핵심 코어.
* **Algorithm:** **PPO (Proximal Policy Optimization)** 또는 **SAC (Soft Actor-Critic)**.
* **Goal (Reward Function):**
    * **Reward (+):** 부상 징후 조기 포착, 정확한 위험도 예측.
    * **Penalty (-):** 불필요한 경기 중단(False Alarm), 컴퓨팅 자원 과다 사용, 놓친 부상(Missed Detection).
* **Reasoning:** "현재 왼쪽 무릎 데이터의 신뢰도가 낮다. 위험 판단을 내리기 전에, 빔을 집중해서 데이터를 더 모으자."라는 식의 추론 수행.

---

## 3. 에이전트의 능동적 행동 (Agentic Actions)

이 시스템이 '알고리즘'이 아니라 '에이전트'인 이유는, 인프라와 환경에 **직접 개입(Intervention)**하기 때문이다. 에이전트는 다음 3가지 Action을 자율적으로 수행한다.

### Action 1: Active Sensing Control (능동적 센싱 제어)
데이터가 부족하거나 사각지대가 발생했을 때, 6G 인프라에 명령을 내린다.
* **Reconfigure RIS:** "선수 7번이 등지고 있어서 심장 박동이 안 들린다. 코너의 RIS(반사판) 각도를 조정해서 우회 경로를 확보하라."
* **Beam Focusing (Zoom-in):** "오른쪽 허벅지 근육 떨림이 의심된다. 전체 스캔을 멈추고, 해당 부위에 **Pencil Beam(초협대역 빔)**을 쏴서 해상도를 10배 높여라."

### Action 2: Dynamic Resource Allocation (자원 동적 할당)
모든 선수를 똑같이 감시하지 않는다. 위험도에 따라 연산 자원을 재배분한다.
* **Priority Shift:** "현재 공을 가지고 전력 질주하는 선수 A, B에게 **MEC 서버 자원의 80%를 할당**하고, 쉬고 있는 선수들은 1초에 한 번만 스캔하라."
* **Latency Control:** 위험 선수의 경우 TTI(Transmission Time Interval)를 0.1ms로 단축시켜 초고속 추적 모드로 전환.

### Action 3: Predictive Intervention (예측적 개입)
사고 발생 후 알림이 아니라, 미래 상태를 시뮬레이션하여 개입한다.
* **Future Simulation:** 현재 관절 각도와 속도를 입력으로 물리 엔진(Physics Engine)을 돌려 **"0.5초 뒤 인대 파열 확률"** 계산.
* **Pre-emptive Alert:** 사고 발생 0.5초 전, 심판에게 **Haptic Feedback(진동)**을 미리 전송하여 휘슬을 불 준비를 시킴.

---

## 4. 의사결정 프로세스 (Markov Decision Process)

에이전트의 사고 과정은 수학적으로 **MDP (Markov Decision Process)**를 따른다.

### Step 1: Observation (관찰) $O_t$
* "선수 #9의 무릎 각도 170도, 햄스트링 떨림 강도 3.5mm, 심부 체온 38.2도."
* *Issue:* 그런데 다른 선수에 가려져서 떨림 데이터의 신뢰도(Confidence)가 60%로 떨어짐.

### Step 2: Policy Decision (정책 판단) $\pi(a|s)$
* **Option A:** 즉시 경고 전송 (오작동 확률 높음 $\rightarrow$ 페널티 예상).
* **Option B:** 무시 (부상 위험 $\rightarrow$ 페널티 예상).
* **Option C:** **Active Sensing 요청 (최적 행동).**

### Step 3: Action Execution (행동 수행) $A_t$
* Agent $\rightarrow$ 6G Infrastructure: **"Beam-forming Matrix 재설정 요청. 선수 #9 다리 방향으로 SNR 20dB 증폭."**

### Step 4: Updated Observation (재관찰) $O_{t+1}$
* 증폭된 신호 수신 결과, 떨림이 3.5mm가 아니라 옷의 흔들림(Noise)이었음이 판명됨.
* 신뢰도 99% 확보.

### Step 5: Final Conclusion (결론)
* **"부상 위험 없음. 경기 지속."** (불필요한 중단 방지 성공 $\rightarrow$ Reward 획득).

---

## 5. 학습 데이터 및 시뮬레이션 (Training & Sim-to-Real)

강화학습 에이전트를 실제 경기장에서 처음부터 학습시킬 수 없으므로, **디지털 트윈 시뮬레이션**을 활용한다.

### 5.1. Digital Twin Environment
* **Engine:** NVIDIA Omniverse / Unity ML-Agents.
* **Scenario:** 가상의 축구 경기장에서 수만 번의 충돌, 넘어짐, 부상 상황을 시뮬레이션.
* **Role of 6G:** Ray-Tracing 기법을 이용해 6G 전파의 반사, 회절, 사각지대 현상을 물리적으로 정확히 구현.

### 5.2. Sim-to-Real Transfer
* 시뮬레이션에서 충분히 학습된 정책(Policy) 모델을 실제 MEC 서버에 배포.
* 실제 경기 중 발생하는 예외 상황(Corner Case)은 **Online Learning**을 통해 실시간으로 모델 업데이트.

---

## 6. 결론: 왜 '에이전트'인가? (Why Agent?)

기존의 시스템이 **"보여주는(Monitoring)"** 역할에 그쳤다면, 본 6G-Guardian은 **"개입하는(Intervening)"** 시스템이다.
에이전트는 불완전한 통신 환경을 스스로 극복하기 위해 **인프라를 제어(Active Sensing)**하고, 제한된 자원을 효율적으로 배분하며, 불확실한 상황에서 최적의 의사결정을 내리는 **자율 주행차 수준의 지능(Autonomy)**을 스포츠 안전 분야에 구현한 것이다.

---

## 7. 참고 문헌 (References)

1.  **"Deep Reinforcement Learning for Beam Management in mmWave/THz Networks"** (IEEE Wireless Comm.)
    * *Core Tech:* 빔포밍 제어를 위한 강화학습 적용 방법론.
2.  **"Proximal Policy Optimization Algorithms"** (OpenAI, 2017)
    * *Core Algo:* 에이전트의 의사결정 모델(PPO).
3.  **"Active Sensing for Robotics"**
    * *Concept:* 정보 획득을 위해 센서를 능동적으로 움직이는 로봇 공학 이론.
4.  **"Task-Oriented Communications for 6G"**
    * *Paradigm:* 데이터 전송 자체가 목적이 아니라, '미션(선수 보호)' 수행을 위해 통신을 도구로 활용하는 6G의 철학.

---
*Created by [Your Name] for 6G-Guardian Project.*
