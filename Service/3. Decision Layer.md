# 📡 Module 03: Autonomous AI Agent & Implementation Spec
> **Project:** 6G-Guardian (ISAC-based Contactless Athlete Protection System)  
> **Module:** AI Implementation, Open-Source Stack, RL Control  
> **Version:** 3.0.0 (Implementation-Ready)  
> **Author:** Team BAESTRO, Mobile Engineering Dept. KNU

---

## 1. 서론 (Introduction)

### 1.1. AI 에이전트의 정의 (Identity)
본 시스템의 Core AI는 단순한 데이터 모니터링 툴이 아니다. 6G 전파를 시각(Vision)처럼 활용하여 경기장 내 모든 선수를 **초정밀 진단**하고, 위험 상황 시 즉각적으로 경기에 **개입(Intervention)** 하는 **'자율형 안전 관리 에이전트(Autonomous Safety Guardian)'** 이다.

### 1.2. 핵심 기능 및 작동 시나리오 (Core Function & Scenario)
이 AI는 **"본다(Observe) $\rightarrow$ 진단한다(Diagnose) $\rightarrow$ 행동한다(Act)"** 의 3단계 루프를 0.1ms 단위로 수행한다.

1.  **Observe (무엇을 보는가?):**
    * 웨어러블 센서 없이 6G 전파 반사파만으로 선수의 **뼈대(Skeleton), 근육 떨림(Tremor), 체온/수분(Vital)** 을 실시간으로 스캔한다.
2.  **Diagnose (무엇을 판단하는가?):**
    * "현재 무릎 각도가 꺾였는가?" (구조적 위험)
    * "근육이 쥐가 나기 직전의 떨림인가?" (생리학적 위험)
    * "열사병으로 쓰러지기 직전인가?" (내과적 위험)
    * 위 3가지를 종합하여 **'0.5초 뒤 부상 발생 확률'** 을 계산한다.
3.  **Act (어떻게 행동하는가?):**
    * **능동적 센싱 (Active Sensing):** 선수가 가려져서 잘 안 보이면, AI가 스스로 기지국 빔(Beam) 방향을 꺾거나 반사판(RIS)을 조절하여 사각지대를 없앤다.
    * **경기 중단 (Game Stop):** 부상 확률이 임계치를 넘으면 심판보다 먼저 판단하여 경고 신호를 보낸다.

### 1.3. 구현 목표 (Implementation Goal)
본 문서는 이러한 에이전트를 구현하기 위해 **검증된 SOTA(State-of-the-Art) 오픈소스 모델**을 어떻게 6G 환경에 맞춰 **최적화(Tuning)** 했는지에 대한 구체적인 엔지니어링 명세를 기술한다.

---

## 2. AI 모델 구현 상세 (Implementation Specification)

4개의 핵심 모듈에 대한 사용 모델, 라이브러리, 튜닝 전략 명세서이다.

### 2.1. Skeletal Kinematics Module (골격 복원)
불완전하고 희소한(Sparse) 6G 레이더 포인트 클라우드를 3D 골격으로 변환한다.

* **Backbone Model:** **PointNet++** (Feature Extractor) + **DGCNN** (Dynamic Graph CNN).
* **Library:** `PyTorch Geometric (PyG)` / `OpenMMLab MMPose`.
* **Engineering Strategy (Knowledge Distillation):**
    * **Problem:** RF 신호는 광학 카메라보다 해상도가 낮아 골격 추출이 어려움.
    * **Solution:** **'Cross-Modal Distillation'** 기법 적용.
        1.  **Teacher (Optical):** `NTU RGB+D 120` 데이터셋으로 학습된 고정밀 카메라 모델.
        2.  **Student (RF):** 6G 레이더 모델.
        3.  **Process:** Teacher가 추출한 Feature Map을 Student가 모방하도록 Loss Function을 설계하여 학습 효율 극대화.

### 2.2. Micro-Tremor Detection Module (떨림 감지)
마이크로 도플러 스펙트로그램 이미지에서 비정상 주파수 패턴을 분류한다.

* **Backbone Model:** **EfficientNet-V2 (Small)**.
    * *Why?* ResNet 대비 파라미터 수가 적어 0.1ms 초저지연 추론(Edge Inference)에 최적화됨.
* **Library:** `Timm (PyTorch Image Models)`.
* **Engineering Strategy (Data Augmentation):**
    * **Problem:** 실제 근육 경련(Cramp) 데이터 확보의 어려움.
    * **Solution:** **'SpecAugment'** 및 **'Contrastive Learning'** 적용.
        1.  **SpecAugment:** 스펙트로그램의 시간/주파수 축을 마스킹(Masking)하여 데이터 증강.
        2.  **SimCLR:** 라벨이 없는 상태에서 '정상 근육 수축'과 '비정상 떨림'의 특징 차이를 스스로 학습(Self-Supervised)하게 한 후 Fine-tuning.

### 2.3. Physiological Inference Module (심부 온도/수분)
데이터가 없는 영역에서도 물리 법칙을 위배하지 않도록 강제한다.

* **Model Architecture:** **PINN (Physics-Informed Neural Network)**.
* **Library:** `DeepXDE` (Scientific ML Library).
* **Engineering Strategy (Physics Regularization):**
    * **Loss Function Design:** 데이터 오차($MSE_{data}$)에 물리 방정식 잔차($MSE_{physics}$)를 추가.
    $$Loss = MSE_{data} + \lambda \cdot || \mathcal{N}_{Pennes}(T) ||^2$$
    * $\mathcal{N}_{Pennes}$: 펜스 생체 열 방정식(Pennes' Bio-heat Eq) 연산자.
    * 이를 통해 피부 온도($T_s$) 데이터만으로 심부 온도($T_b$)를 정확히 역산출.

### 2.4. Autonomous Control Agent (강화학습 제어)
상황에 따라 6G 빔포밍을 제어하고 경기를 중단시키는 의사결정체.

* **Algorithm:** **PPO (Proximal Policy Optimization)**.
    * *Why?* DQN 대비 학습 안정성이 높고, 연속적인 제어(Continuous Action Space)에 유리함.
* **Library:** `Stable Baselines3` (RL Algorithm) / `Ray RLLib` (Distributed Training).
* **Environment:** `Unity ML-Agents` (Sim-to-Real).
* **Reward System:**
    * **(+) Reward:** 부상 징후 조기 포착 (+10), 정확한 위험도 예측 (+5).
    * **(-) Penalty:** 오작동/False Alarm (-5), 부상 놓침 (-20), 컴퓨팅 자원 과다 사용 (-1).

---

## 3. 에이전트 루프 및 의사결정 (Agent Loop & MDP)

에이전트는 **MDP (Markov Decision Process)** 기반의 폐루프(Closed-loop) 제어를 수행한다.

### Step 1: Observation ($S_t$)
* 4개 모듈(골격, 떨림, 체온, 수분)의 출력 벡터 및 신뢰도(Confidence Score) 수집.
* 상태 벡터 $S_t = [v_{skel}, v_{tremor}, v_{temp}, v_{hyd}, c_{conf}]$.

### Step 2: Policy Reasoning ($A_t \sim \pi(S_t)$)
* PPO 정책 신경망이 현재 상태에서 최적의 행동을 확률적으로 선택.
* **Case A (불확실):** 신뢰도 $c_{conf} < 0.6$ $\rightarrow$ **Action: Active Sensing** (빔포밍 집중, RIS 각도 변경).
* **Case B (위험 감지):** 위험도 $R_{total} > 0.9$ $\rightarrow$ **Action: Game Stop** (심판 알림 전송).

### Step 3: Execution & Feedback
* 선택된 행동을 6G 인프라(RU/MEC)에 명령하고, 변화된 환경($S_{t+1}$)을 다시 관측.

---

## 4. 위험도 산출 로직 (Total Risk Scoring)

최종 판단을 위한 정량적 지표 계산식이다.

$$R_{total}(t) = w_1 \cdot R_{kin}(t) + w_2 \cdot R_{trem}(t) + w_3 \cdot R_{vit}(t)$$

* **$R_{kin}$ (ST-GCN Output):** 관절 과신전 및 비정상 충격량 (0.0~1.0).
* **$R_{trem}$ (EfficientNet Output):** 병리학적 떨림 확률 (Softmax Score).
* **$R_{vit}$ (PINN Output):** 열사병($T_b > 39.5^\circ C$) 및 탈수($\text{Loss} > 2\%$) 위험도.
* **Adaptive Weights ($w_i$):** 강화학습 에이전트가 상황(Context)에 따라 가중치를 동적으로 조절. (예: 충돌 직후엔 $w_1$ 증가, 폭염 시엔 $w_3$ 증가).

---

## 5. 능동적 센싱 제어 메커니즘 (Active Sensing Control Loop)

**"AI의 판단에 따라 물리 계층(PHY)을 실시간으로 재구성하는 기술적 절차"**

AI Agent(Module 03)가 "데이터 불충분" 또는 "정밀 스캔 필요"를 요청했을 때, 인프라가 이를 수행하는 구체적인 메커니즘이다. 이 제어 루프는 **O-RAN의 Near-RT RIC (Near-Real-Time RAN Intelligent Controller)** 에서 수행된다.

#### A. Beam Focusing Control (빔 포커싱 제어)
* **Trigger:** "선수 #7의 왼쪽 무릎 관절 데이터의 신뢰도(Confidence)가 낮음."
* **Action:**
    1.  **Target Update:** RIC가 해당 선수의 최신 3D 좌표 $(x, y, z)$를 타겟으로 설정.
    2.  **Precoding Calculation:** 분산된 D-AP들이 타겟 지점에 에너지를 집중하도록 **빔포밍 가중치 벡터($\mathbf{w}_k$)** 를 재계산.
        * *Objective:* Maximize SNR at $(x, y, z)$.
    3.  **Execution:** 10ms 이내에 D-AP의 위상 천리기(Phase Shifter)를 조절하여 **'Pencil Beam(초협대역 빔)'** 형성.

#### B. RIS Configuration Update (반사판 경로 제어)
* **Trigger:** "선수 #9가 다른 선수에 가려져 신호가 끊김 (NLoS 상황)."
* **Action:**
    1.  **Path Planning:** AI가 주변의 RIS 중 가용 가능한 패널(RIS #4)을 식별.
    2.  **Phase Optimization:** 입사각($\theta_{in}$)과 반사각($\theta_{out}$)을 계산하여, RIS 소자들의 **위상 제어 매트릭스($\mathbf{\Phi}$)** 최적화.
    3.  **Actuation:** RIS 컨트롤러에 제어 신호를 보내 물리적으로 전파 경로를 우회(Detour)시킴.

#### C. Waveform Adaptation (파형 적응 제어)
* **Trigger:** "미세 떨림(Tremor) 감지를 위해 더 높은 해상도가 필요함."
* **Action:**
    1.  **Pilot Density Increase:** ISAC 프레임 내에서 센싱용 파일럿 심볼의 밀도를 일시적으로 2배 증강.
    2.  **Bandwidth Expansion:** 해당 선수에게 할당된 주파수 대역폭(B)을 순간적으로 확장하여 거리 분해능($\Delta R = c/2B$) 향상.

---

## 6. 결론 (Conclusion)

본 Decision Layer는 단순한 이론적 모델링이 아닌, **검증된 오픈소스 라이브러리(PyG, Timm, SB3)** 와 **최신 학습 기법(Knowledge Distillation, PINN)** 을 결합하여 설계되었다.
특히 **강화학습 기반의 능동 제어(Active Control)** 기능은 에이전트가 불안정한 무선 통신 환경을 스스로 극복하고, 인간 수준의 상황 판단을 내릴 수 있게 하는 핵심 차별점이다.

---

## 7. 참고 문헌 (References)

1.  **"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"** (NIPS 2017).
2.  **"EfficientNetV2: Smaller Models and Faster Training"** (ICML 2021).
3.  **"Physics-Informed Neural Networks: A Deep Learning Framework"** (J. Comput. Phys., 2019).
4.  **"Proximal Policy Optimization Algorithms"** (OpenAI, 2017).
5.  **"Cross-Modal Knowledge Distillation for Vision-to-Sensor Transfer"** (CVPR).

---
*Created by [Your Name] for 6G-Guardian Project.*
